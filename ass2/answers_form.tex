\documentclass[letterpaper,11pt]{article}
\title{CS 6362 Machine Learning, Fall 2017: Homework 2}
\date{}
\author{\bf Venkataramana Nagarajan}


\usepackage[margin=1in]{geometry}
% \usepackage{hyperref}
\usepackage[colorlinks]{hyperref}
\usepackage{capt-of}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{bbm}
\usepackage{float}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{hyperref} 
\usepackage{color}
\usepackage{amstext}
\usepackage{enumerate}
\usepackage{amsmath,bm}
\usepackage{fullpage}
    
\renewcommand{\baselinestretch}{1.15}    

\begin{document}

\maketitle

\paragraph{Question 1:}

\paragraph{Question 2:}


\paragraph{Question 3:}
\begin{enumerate}[(a)]
\item

\item
\end{enumerate}

\paragraph{Question 4:}
\begin{enumerate}[(a)]
\item

\item

\item

\end{enumerate}

\paragraph{Question 5:}
\begin{enumerate}[(a)]
\item

\item
	
\end{enumerate}

\paragraph{Question 2:}

Since the cross-validation error is the same for both set of parameters. This question can be answered with the following reasoning:
\begin{itemize}
	\item It is dependant how many more support vectors are being generated by the second set of parameters. Higher number of support vectors lead for a higher chance of over-fitting.
\end{itemize}

\begin{itemize}
	\item The computational time involved in prediction for these case of parameters, it is known that predictions will be computationally faster with lesser number of support vectors. Hence they would be preferable.
\end{itemize}

\begin{itemize}
	\item An analogy can also be taken from Occam's razor that the simpler solution is always preferable as these vectors can be considered as generated assumptions by the model for giving predictions. This would again lead to a choice of parameters which provided lesser number of support vectors.
	
\end{itemize}

\paragraph{Question 3:}

\paragraph{Question 4:}


a) In a polynomial kernel increasing 'd' which is the number of dimensions will increase the chances of over-fitting. The reason being that as 'd' is increased the number of features present in the higher dimensional space will increase drastically. Higher dimension projection of the existing feature space not only make the model overfit but also they make it computationally expensive to perform. Hence this kernel suffers from the curse of dimensionality.


b) The Gaussian kernel is not directly affected by increasing dimensions. While the sigma governs the estimation for bias and variance. The affect of sigma is not directly relatable to curse of dimensionality because the expansion of this kernel is a taylor series and that is infinite all scenarios eitherways. Hence increasing or decreasing sigma will not be a cause for over-fitting however it can be used to control scenarios of bias and variance for the model.


c) Using the assumption that the feature maps are not infinite

Since: \begin{equation} K(x_i,x_i') = <\phi(x_i),\phi(x_i')> \end{equation}

Since H(a) = max(1-a,0) it can be inferred that H(a) >= 0 for all scenarios.

If 1-a>0 then:

\begin{equation}
a = y_i(w^t x_i)
\end{equation}

This is representative of a linear function. If we suppose a scenario for two functions are present in a max function and it can be proven that the max is a convex then it can be said the same for the function above.

Consider two generic functions as h = max(f,g)

Linear inequality for convexity: \begin{equation}h(z) <= t h(x)+(1 − t) h(y) h(z) <= t h (x) + (1 − t) h(y) where, z = t x+(1 − t) y z = t x + (1 − t) y
\end{equation}

Since,

\begin{equation}h=max{f,g}
\end{equation}

This is equivalent to two inequalities:
\begin{equation}
f(z)<=t h(x)+(1−t)h(y)
\end{equation}

\begin{equation}
g(z)<=t h(x)+(1−t)h(y)
\end{equation}

By convexity of f one knows that:

\begin{equation}
f(z)<=t f(x)+(1−t)f(y)f(z)<=t f(x)+(1−t)f(y)
\end{equation}
Therefore we can say that H(a) is a convex function of a.



\end{document}